import os
import pandas as pd
import streamlit as st
from sentence_transformers import SentenceTransformer
import chromadb
from openai import OpenAI

# --- Telemetryç„¡åŠ¹åŒ–ï¼ˆChromaDBãŒèµ·å‹•æ™‚ã«å‚ç…§ï¼‰ ---
os.environ["CHROMADB_TELEMETRY_ENABLED"] = "False"

# --- OpenAIåˆæœŸåŒ–ï¼ˆStreamlit Cloudã®secretsã‹ã‚‰å–å¾—ï¼‰ ---
client_ai = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# --- ChromaDBï¼ˆã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
client = chromadb.Client()
collection = client.get_or_create_collection(
    name="line_rangers",
    metadata={"hnsw:space": "cosine"}  # âœ… ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æ¤œç´¢
)

# --- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰ ---
@st.cache_resource
def load_embedder():
    return SentenceTransformer("all-MiniLM-L6-v2")

embedder = load_embedder()

# --- åˆæœŸãƒ‡ãƒ¼ã‚¿ç™»éŒ²ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰ ---
@st.cache_data
def load_and_register():
    df = pd.read_csv("scraped.csv")

    # ã‚¿ã‚¤ãƒˆãƒ«ï¼‹æœ¬æ–‡ã‚’é€£çµã—ã¦æ–‡æ›¸ã«ã™ã‚‹
    if "title" in df.columns and "body" in df.columns:
        texts = [f"{row['title']}ï¼š{row['body']}" for _, row in df.iterrows()]
    elif "text" in df.columns:
        texts = df["text"].tolist()
    else:
        raise ValueError("CSVã« 'title'+'body' ã‹ 'text' ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")

    embeddings = embedder.encode(texts).tolist()
    collection.add(
        documents=texts,
        embeddings=embeddings,
        ids=[f"id{i}" for i in range(len(texts))]
    )
    return len(texts)

num_docs = load_and_register()

# --- é¡ä¼¼æ–‡æ›¸ã®æ¤œç´¢ã¨æ–‡è„ˆä½œæˆ ---
def create_context(question: str, max_len: int = 1800) -> str:
    q_embedding = embedder.encode([question])[0]
    results = collection.query(query_embeddings=[q_embedding], n_results=7)

    texts = []
    total_words = 0
    for doc in results["documents"][0]:
        word_count = len(doc.split())
        if total_words + word_count > max_len:
            break
        texts.append(doc)
        total_words += word_count

    return "\n\n###\n\n".join(texts)

# --- GPTã§è³ªå•ã«å›ç­” ---
def answer_question(question: str, history: list) -> str:
    context = create_context(question, max_len=200)
    prompt = f"""
ã‚ãªãŸã¯ã‚²ãƒ¼ãƒ ã®æ”»ç•¥æƒ…å ±ç™ºä¿¡è€…ã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

---
è³ªå•: {question}
å›ç­”:
""".strip()

    history.append({"role": "user", "content": prompt})
    try:
        response = client_ai.chat.completions.create(
            model="gpt-4o",
            messages=history,
            temperature=0.7,
        )
        answer = response.choices[0].message.content.strip()
        history.append({"role": "assistant", "content": answer})
        return answer
    except Exception as e:
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}"

# --- Streamlit UI ---
st.set_page_config(page_title="LINEãƒ¬ãƒ³ã‚¸ãƒ£ãƒ¼Q&A", page_icon="ğŸ®")
st.title("ğŸ® LINEãƒ¬ãƒ³ã‚¸ãƒ£ãƒ¼Q&A ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

st.info(f"ğŸ“š ç™»éŒ²æ¸ˆã¿æ–‡æ›¸æ•°: {num_docs} ä»¶")

if "history" not in st.session_state:
    st.session_state.history = []

question = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")

if st.button("é€ä¿¡") and question:
    with st.spinner("è€ƒãˆä¸­..."):
        answer = answer_question(question, st.session_state.history)
        st.markdown("---")
        st.markdown(f"**ã‚ãªãŸã®è³ªå•ï¼š** {question}")
        st.markdown(f"**AIã®å›ç­”ï¼š** {answer}")
